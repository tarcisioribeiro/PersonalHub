services:
  # PostgreSQL Database with pgvector extension
  db:
    image: pgvector/pgvector:pg16
    container_name: personalhub-db
    environment:
      POSTGRES_DB: ${DB_NAME:-personalhub_db}
      POSTGRES_USER: ${DB_USER}
      POSTGRES_PASSWORD: ${DB_PASSWORD}
      POSTGRES_INITDB_ARGS: "--encoding=UTF8 --locale=C.UTF-8"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./backups:/backups
      - ./api/init-pgvector.sql:/docker-entrypoint-initdb.d/init-pgvector.sql
    ports:
      - "${DB_PORT:-39102}:5432"
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${DB_USER} -d ${DB_NAME:-personalhub_db}"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    networks:
      - personalhub-network

  # Redis for semantic caching
  redis:
    image: redis:7-alpine
    container_name: personalhub-redis
    ports:
      - "${REDIS_PORT:-6379}:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes --maxmemory 256mb --maxmemory-policy allkeys-lru
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - personalhub-network

  # Ollama for local LLM inference and embeddings
  ollama:
    image: ollama/ollama:latest
    container_name: personalhub-ollama
    ports:
      - "${OLLAMA_PORT:-11435}:11435"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0:11435
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    networks:
      - personalhub-network

  # Inicializador do Ollama - baixa modelos necessarios
  ollama-init:
    image: curlimages/curl:latest
    container_name: personalhub-ollama-init
    depends_on:
      ollama:
        condition: service_healthy
    environment:
      - OLLAMA_HOST=ollama
      - OLLAMA_PORT=11435
      - OLLAMA_EMBED_MODEL=${OLLAMA_EMBED_MODEL:-nomic-embed-text}
      - OLLAMA_LLM_MODEL=${OLLAMA_LLM_MODEL:-mistral:7b}
    volumes:
      - ./ollama-init.sh:/ollama-init.sh:ro
    entrypoint: ["/bin/sh", "/ollama-init.sh"]
    restart: "no"
    networks:
      - personalhub-network

  # Django API Backend
  api:
    build:
      context: ./api
      dockerfile: Dockerfile
    container_name: personalhub-api
    ports:
      - "${API_PORT:-39100}:${API_PORT:-39100}"
    env_file:
      - .env
    environment:
      - DB_HOST=db
      - DB_PORT=5432
      - LOG_FORMAT=${LOG_FORMAT:-json}
      - PYTHONUNBUFFERED=1
      - DJANGO_SETTINGS_MODULE=app.settings
      - REDIS_URL=redis://redis:6379/0
      - OLLAMA_URL=http://ollama:11435
      - OLLAMA_EMBED_MODEL=nomic-embed-text
      - OLLAMA_LLM_MODEL=mistral:7b
    depends_on:
      db:
        condition: service_healthy
      redis:
        condition: service_healthy
      ollama:
        condition: service_healthy
    volumes:
      - ./api:/app
      - ./api/media:/app/media
      - ./api/logs:/app/logs
      - ./api/staticfiles:/app/staticfiles
    restart: unless-stopped
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "curl -f http://localhost:${API_PORT:-39100}/health/ || wget -q --spider http://localhost:${API_PORT:-39100}/health/",
        ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    networks:
      - personalhub-network

  # React Frontend
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
      args:
        - VITE_API_BASE_URL=${VITE_API_BASE_URL:-http://localhost:39100}
    container_name: personalhub-frontend
    ports:
      - "${FRONTEND_PORT:-39101}:80"
    depends_on:
      - api
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:80 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
    networks:
      - personalhub-network

volumes:
  postgres_data:
    driver: local
  redis_data:
    driver: local
  ollama_data:
    driver: local

networks:
  personalhub-network:
    driver: bridge
